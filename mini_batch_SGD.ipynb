{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "plt.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "#Function y = 4*x1*x1 + 9*x2*x2   \n",
    "def get_data(sample_num=1000):\n",
    "    x1 = np.linspace(0, 9, sample_num)  \n",
    "    x2 = np.linspace(4, 13, sample_num)\n",
    "    x = np.concatenate(([x1], [x2]), axis=0).T   # generate a new array\n",
    "    x = x*x\n",
    "    y = np.dot(x, np.array([4, 9]).T)   \n",
    "    return x, y\n",
    "\n",
    "\n",
    "def shuffle_data(x, y):#Randomly shuffle x, y data\n",
    "    seed = random.random()\n",
    "    random.seed(seed)\n",
    "    random.shuffle(x)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(y)\n",
    "\n",
    "\n",
    "def get_splited_data(x, y, ratio):\n",
    "    shuffle_data(x, y)\n",
    "    train_size = int (len(x)*ratio)\n",
    "    test_size = int (len(x)*(1-ratio))\n",
    "    train_x = x[:train_size]\n",
    "    train_y = y[:train_size]\n",
    "    test_x = x[test_size:]\n",
    "    test_y = y[test_size:]\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "# Calculate the gradient\n",
    "def compute_grad(X,y,batch_size,theta,type):\n",
    "    row = X.shape[0]\n",
    "    col = X.shape[1]\n",
    "    grad = np.ones((col, 1))\n",
    "    if type == 'FULL':  # GD\n",
    "        h = np.dot(X, theta)\n",
    "        grad = np.dot(np.transpose(X), h-y)/row\n",
    "    elif type == 'SGD':  # SGD\n",
    "        r = np.random.randint(row)                            # Randomly select a sample from all samples\n",
    "        h = np.dot(np.array([X[r, :]]), theta)\n",
    "        grad = np.dot(np.transpose(np.array([X[r, :]])), h - np.array([y[r, :]]))\n",
    "\n",
    "    elif type == 'MINI':                                                                             # Mini-Batch \n",
    "        r = np.random.choice(row,batch_size,replace=False)  # choose  a batch sample\n",
    "        h = np.dot(X[r,:], theta)\n",
    "        grad = np.dot(np.transpose(X[r,:]), h - y[r,:]) / batch_size\n",
    "    else:\n",
    "        print(\"NO such gradient dencent Method!\")\n",
    "\n",
    "    # Calculatie train_loss\n",
    "    loss = compute_RMSE(X, y, theta)\n",
    "    return grad, loss\n",
    "\n",
    "\n",
    "# Update theta\n",
    "def update_theta(grad,theta,alpha):\n",
    "    theta = theta - alpha*grad\n",
    "    return theta\n",
    "\n",
    "\n",
    "# LMS as loss function\n",
    "def compute_RMSE(X,y,theta):\n",
    "    row = X.shape[0]\n",
    "    hh = np.dot(X, theta)\n",
    "    RMSE = np.dot((np.transpose(hh-y)),(hh-y))/(2*row)    \n",
    "    return RMSE\n",
    "\n",
    "\n",
    "# run Mini-batch SGD\n",
    "def run_mini_sgd():\n",
    "\n",
    "    # Obtain the data set and divide it into training and test sets\n",
    "    X, y = get_data()\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    shuffle_data(X,y)\n",
    "    train_x,train_y,test_x,test_y = get_splited_data(X, y, ratio=0.8)\n",
    "\n",
    "    col = X.shape[1]  #\n",
    "    theta = np.zeros((col, 1))\n",
    "    max_step = 500   #The maximum number of iterations\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_step = []\n",
    "    test_step = []\n",
    "    loss = 1\n",
    "    step = 0\n",
    "    while loss > 0.01 and step < max_step:\n",
    "\n",
    "        grad, loss = compute_grad(train_x, train_y, batch_size=500, theta=theta, type='MINI')\n",
    "        theta = update_theta(grad, theta, alpha=0.001)\n",
    "        print(\"step: \", step, \"loss:\", loss[0][0])\n",
    "\n",
    "        train_loss.append(loss[0][0])\n",
    "        train_step.append(step)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            testloss = compute_RMSE(test_x, test_y, theta)\n",
    "            test_loss.append(testloss[0][0])\n",
    "            test_step.append(step)\n",
    "\n",
    "        step += 1\n",
    "    print(theta)\n",
    "    # plot loss_curve\n",
    "    plt.plot(train_step, train_loss, c='red', label='train_loss_curve')\n",
    "    plt.plot(test_step, test_loss, c='blue', label='test_loss_curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"loss_curve\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_mini_sgd()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
